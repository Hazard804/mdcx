import asyncio
import re
from collections import defaultdict
from collections.abc import Sequence
from typing import override

from parsel import Selector
from patchright._impl._api_structures import SetCookieParam
from patchright.async_api import Browser

from mdcx.base.web import check_url
from mdcx.config.manager import manager
from mdcx.config.models import Website
from mdcx.models.types import CrawlerInput
from mdcx.signals import signal
from mdcx.utils.dataclass import update_valid
from mdcx.utils.gather_group import GatherGroup
from mdcx.web_async import AsyncWebClient

from ..base import (
    Context,
    CralwerException,
    CrawlerData,
    DetailPageParser,
    GenericBaseCrawler,
    is_valid,
)
from .parsers import Category, DigitalParser, MonoParser, RentalParser, parse_category
from .tv import DmmTvResponse, FanzaResp, dmm_tv_com_payload, fanza_tv_payload


class DMMContext(Context):
    number_00: str | None = None
    number_no_00: str | None = None


class DmmCrawler(GenericBaseCrawler[DMMContext]):
    mono = MonoParser()
    digital = DigitalParser()
    rental = RentalParser()

    def __init__(self, client: AsyncWebClient, base_url: str = "", browser: Browser | None = None):
        super().__init__(client, base_url, browser)

    async def _http_request_with_retry(self, method: str, url: str, **kwargs):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„ HTTP è¯·æ±‚

        Args:
            method: HTTP æ–¹æ³• ('GET', 'POST', 'HEAD')
            url: è¯·æ±‚ URL
            **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°

        Returns:
            (response, error) å…ƒç»„
        """
        max_retries = manager.config.retry  # ä»é…ç½®è·å–é‡è¯•æ¬¡æ•°

        last_error = None

        for attempt in range(max_retries + 1):
            try:
                if method.upper() == "POST":
                    if "json_data" in kwargs:
                        response, error = await self.async_client.post_json(url, **kwargs)
                    else:
                        response, error = await self.async_client.post_text(url, **kwargs)
                elif method.upper() == "GET":
                    response, error = await self.async_client.get_text(url, **kwargs)
                elif method.upper() == "HEAD":
                    response, error = await self.async_client.request("HEAD", url, **kwargs)
                else:
                    response, error = await self.async_client.request(method, url, **kwargs)

                # å¦‚æœè¯·æ±‚æˆåŠŸï¼Œç›´æ¥è¿”å›
                if response is not None:
                    return response, error

                # è®°å½•å¤±è´¥ä¿¡æ¯
                last_error = error

            except Exception as e:
                last_error = str(e)

            # é‡è¯•å‰ç­‰å¾…ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            if attempt < max_retries:
                wait_time = min(2**attempt, 10)  # æœ€å¤šç­‰å¾…10ç§’
                await asyncio.sleep(wait_time)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        return None, f"è¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {last_error}"

    @classmethod
    @override
    def site(cls) -> Website:
        return Website.DMM

    @classmethod
    @override
    def base_url_(cls) -> str:
        # DMM ä¸æ”¯æŒè‡ªå®šä¹‰ URL
        return ""

    @override
    def new_context(self, input: CrawlerInput) -> DMMContext:
        return DMMContext(input=input)

    @override
    def _get_cookies(self, ctx) -> dict[str, str] | None:
        return {"age_check_done": "1"}

    @override
    def _get_cookies_browser(self, ctx: DMMContext) -> Sequence[SetCookieParam] | None:
        return [
            SetCookieParam(name="age_check_done", value="1", domain=".dmm.co.jp", path="/"),
            SetCookieParam(name="age_check_done", value="1", domain=".dmm.com", path="/"),
        ]

    @override
    async def _generate_search_url(self, ctx) -> list[str] | None:
        number = ctx.input.number.lower()

        if x := re.findall(r"[A-Za-z]+-?(\d+)", number):
            digits = x[0]
            if len(digits) >= 5 and digits.startswith("00"):
                number = number.replace(digits, digits[2:])
            elif len(digits) == 4:
                number = number.replace("-", "0")  # https://github.com/sqzw-x/mdcx/issues/393

        # æœç´¢ç»“æœå¤šï¼Œä½†snis-027æ²¡ç»“æœ
        number_00 = number.replace("-", "00")
        # æœç´¢ç»“æœå°‘
        number_no_00 = number.replace("-", "")
        ctx.number_00 = number_00
        ctx.number_no_00 = number_no_00

        return [
            f"https://www.dmm.co.jp/search/=/searchstr={number_00}/sort=ranking/",
            f"https://www.dmm.co.jp/search/=/searchstr={number_no_00}/sort=ranking/",
            f"https://www.dmm.com/search/=/searchstr={number_no_00}/sort=ranking/",  # å†™çœŸ
        ]

    @override
    async def _parse_search_page(self, ctx, html, search_url) -> list[str] | None:
        if "404 Not Found" in html.css("span.d-txten::text").get(""):
            raise CralwerException("404! é¡µé¢åœ°å€é”™è¯¯ï¼")

        # \"detailUrl\":\"https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=ssni00103/?i3_ord=1\u0026i3_ref=search"
        url_list = set(html.re(r'detailUrl\\":\\"(.*?)\\"'))
        if not url_list:
            ctx.debug(f"æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœ: {ctx.input.number} {search_url=}")
            return None

        number_parts: re.Match[str] | None = re.search(r"(\d*[a-z]+)?-?(\d+)", ctx.input.number.lower())
        if not number_parts:
            ctx.debug(f"æ— æ³•ä»ç•ªå· {ctx.input.number} æå–å‰ç¼€å’Œæ•°å­—")
            return None
        prefix = number_parts.group(1)
        digits = number_parts.group(2)
        n1 = f"{prefix}{digits:0>5}"
        n2 = f"{prefix}{digits}"

        res = []
        for u in url_list:
            # https://tv.dmm.co.jp/list/?content=mide00726&i3_ref=search&i3_ord=1
            # https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=mide00726/?i3_ref=search&i3_ord=2
            # https://www.dmm.com/mono/dvd/-/detail/=/cid=n_709mmrak089sp/?i3_ref=search&i3_ord=1
            if re.search(rf"[^a-z]{n1}[^0-9]", u) or re.search(rf"[^a-z]{n2}[^0-9]", u):
                res.append(u.encode("utf-8").decode("unicode_escape"))

        return res

    @classmethod
    def _get_parser(cls, category: Category):
        match category:
            case Category.PRIME | Category.MONTHLY | Category.MONO:
                return cls.mono
            case Category.DIGITAL:
                return cls.digital
            case Category.RENTAL:
                return cls.rental

    @override
    async def _detail(self, ctx: DMMContext, detail_urls: list[str]) -> CrawlerData | None:
        d = defaultdict(list)
        for url in detail_urls:
            category = parse_category(url)
            d[category].append(url)

        # è®¾ç½® GatherGroup çš„æ•´ä½“è¶…æ—¶æ—¶é—´ï¼Œç»™å•ä¸ªè¯·æ±‚æ›´å¤šæ—¶é—´
        # å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å•ä¸ªè¯·æ±‚ä¸­å®ç°äº†é‡è¯•æœºåˆ¶
        total_timeout = manager.config.timeout * (manager.config.retry + 1) * 2  # ç»™è¶³å¤Ÿçš„æ—¶é—´

        async with GatherGroup[CrawlerData](timeout=total_timeout) as group:
            for url in d[Category.FANZA_TV]:
                group.add(self.fetch_fanza_tv(ctx, url))
            for url in d[Category.DMM_TV]:
                group.add(self.fetch_dmm_tv(ctx, url))

            for category in (
                Category.DIGITAL,
                Category.MONO,
                Category.RENTAL,
                Category.PRIME,
                Category.MONTHLY,
            ):  # ä¼˜å…ˆçº§
                parser = self._get_parser(category)
                if parser is None:
                    continue
                for u in sorted(d[category]):
                    group.add(self.fetch_and_parse(ctx, u, parser))

        res = None
        for r in group.results[::-1]:
            if isinstance(r, Exception):  # é¢„è®¡åªä¼šè¿”å›ç©ºå€¼, ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
                ctx.debug(f"é¢„æ–™ä¹‹å¤–çš„å¼‚å¸¸: {r}")
                continue
            if res is None:
                res = r
            else:
                res = update_valid(res, r, is_valid)

        return res

    async def fetch_fanza_tv(self, ctx: Context, detail_url: str) -> CrawlerData:
        cid = re.search(r"content=([^&/]+)", detail_url)
        if not cid:
            ctx.debug(f"æ— æ³•ä» DMM TV URL æå– cid: {detail_url}")
            return CrawlerData()
        cid = cid.group(1)

        # ä½¿ç”¨å¸¦é‡è¯•çš„ HTTP è¯·æ±‚
        response, error = await self._http_request_with_retry(
            "POST", "https://api.tv.dmm.co.jp/graphql", json_data=fanza_tv_payload(cid)
        )
        if response is None:
            ctx.debug(f"Fanza TV API è¯·æ±‚å¤±è´¥: {cid=} {error=}")
            return CrawlerData()
        try:
            resp = FanzaResp.model_validate(response)
            data = resp.data.fanzaTvPlus.content
        except Exception as e:
            ctx.debug(f"Fanza TV API å“åº”è§£æå¤±è´¥: {e}")
            return CrawlerData()

        extrafanart = []
        for sample_pic in data.samplePictures:
            if sample_pic.imageLarge:
                extrafanart.append(sample_pic.imageLarge)

        # https://cc3001.dmm.co.jp/hlsvideo/freepv/s/ssi/ssis00497/playlist.m3u8
        trailer_url = data.sampleMovie.url.replace("hlsvideo", "litevideo")

        # æ£€æµ‹æ˜¯å¦ä¸ºä¸´æ—¶é“¾æ¥ï¼ˆåŒ…å« /pv/ è·¯å¾„çš„ä¸´æ—¶ URLï¼‰
        # ä¸´æ—¶é“¾æ¥ç¤ºä¾‹: https://cc3001.dmm.co.jp/pv/{temp_key}/{filename}
        # æ”¯æŒå¤šç§æ ¼å¼ï¼šasfb00192_mhb_w, 1start4814k, n_707agvn001_dmb_w ç­‰
        if "/pv/" in trailer_url:
            # ä»ä¸´æ—¶é“¾æ¥ä¸­æå–æ–‡ä»¶å
            filename_match = re.search(r"/pv/[^/]+/(.+?)(?:\.mp4)?$", trailer_url)
            if filename_match:
                filename_base = filename_match.group(1).replace(".mp4", "")
                # å»æ‰è´¨é‡æ ‡è®°åç¼€ï¼ˆ_*b_w æ ¼å¼ï¼Œå¦‚ _mhb_w, _dmb_w, _sm_w, _dm_w ç­‰ï¼‰
                cid = re.sub(r"(_[a-z]+b?_w)?$", "", filename_base)
                # ç¡®ä¿æå–åˆ°çš„æ˜¯æœ‰æ•ˆçš„äº§å“IDï¼ˆåŒ…å«å­—æ¯å’Œæ•°å­—ï¼‰
                if re.search(r"[a-z]", cid, re.IGNORECASE) and re.search(r"\d", cid):
                    # æ„å»ºæ ‡å‡†æ ¼å¼çš„é“¾æ¥
                    # æ ¼å¼: /litevideo/freepv/{prefix}/{three_char}/{full_number}/{filename}.mp4
                    prefix = cid[0]  # ç¬¬ä¸€ä¸ªå­—ç¬¦
                    three_char = cid[:3]  # å‰ä¸‰ä¸ªå­—ç¬¦
                    # ä½¿ç”¨åŸå§‹æ–‡ä»¶åä½†æ›¿æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    trailer = (
                        f"https://cc3001.dmm.co.jp/litevideo/freepv/{prefix}/{three_char}/{cid}/{filename_base}.mp4"
                    )
                else:
                    trailer = ""
            else:
                trailer = ""
        else:
            # åŸæœ‰çš„æ ‡å‡†é“¾æ¥å¤„ç†é€»è¾‘
            cid_match = re.search(r"/([^/]+)/playlist.m3u8", trailer_url)
            if cid_match:
                cid = cid_match.group(1)
                trailer = trailer_url.replace("playlist.m3u8", cid + "_sm_w.mp4")
            else:
                trailer = ""

        return CrawlerData(
            title=data.title,
            outline=data.description,
            release=data.startDeliveryAt,  # 2025-05-17T20:00:00Z
            tags=[genre.name for genre in data.genres],
            runtime=str(int(data.playInfo.duration / 60)),
            actors=[a.name for a in data.actresses],
            poster=data.packageImage,
            thumb=data.packageLargeImage,
            score=str(data.reviewSummary.averagePoint),
            series=data.series.name,
            directors=[d.name for d in data.directors],
            studio=data.maker.name,
            publisher=data.label.name,
            extrafanart=extrafanart,
            trailer=trailer,
            external_id=detail_url,
        )

    async def fetch_dmm_tv(self, ctx: Context, detail_url: str) -> CrawlerData:
        season_id = re.search(r"seasonId=(\d+)", detail_url)
        if not season_id:
            ctx.debug(f"æ— æ³•ä» DMM TV URL æå– seasonId: {detail_url}")
            return CrawlerData()
        season_id = season_id.group(1)

        # ä½¿ç”¨å¸¦é‡è¯•çš„ HTTP è¯·æ±‚
        response, error = await self._http_request_with_retry(
            "POST", "https://api.tv.dmm.com/graphql", json_data=dmm_tv_com_payload(season_id)
        )
        if response is None:
            ctx.debug(f"DMM TV API è¯·æ±‚å¤±è´¥: {season_id=} {error=}")
            return CrawlerData()
        try:
            resp = DmmTvResponse.model_validate(response)
            data = resp.data.video
        except Exception as e:
            ctx.debug(f"DMM TV API å“åº”è§£æå¤±è´¥: {e}")
            return CrawlerData()

        studio = ""
        if r := [item.staffName for item in data.staffs if item.roleName in ["åˆ¶ä½œãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³", "åˆ¶ä½œ", "åˆ¶ä½œè‘—ä½œ"]]:
            studio = r[0]

        return CrawlerData(
            title=data.titleName,
            outline=data.description,
            actors=[item.actorName for item in data.casts],
            poster=data.packageImage,
            thumb=data.keyVisualImage,
            tags=[item.name for item in data.genres],
            release=data.startPublicAt,  # 2025-05-17T20:00:00Z
            year=str(data.productionYear),
            score=str(data.reviewSummary.averagePoint),
            directors=[item.staffName for item in data.staffs if item.roleName == "ç›£ç£"],
            studio=studio,
            publisher=studio,
            external_id=detail_url,
        )

    async def fetch_and_parse(self, ctx: DMMContext, detail_url: str, parser: DetailPageParser) -> CrawlerData:
        html, error = await self._fetch_detail(ctx, detail_url)
        if html is None:
            ctx.debug(f"è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {error=}")
            return CrawlerData()
        ctx.debug(f"è¯¦æƒ…é¡µè¯·æ±‚æˆåŠŸ: {detail_url=}")
        return await parser.parse(ctx, Selector(html), external_id=detail_url)

    @override
    async def _fetch_detail(self, ctx: DMMContext, url: str, use_browser=None) -> tuple[str | None, str]:
        if parse_category(url) not in (Category.DIGITAL):
            return await super()._fetch_detail(ctx, url, False)  # å¯¹äºç¡®å®šä¸éœ€è¦æµè§ˆå™¨çš„, å¼ºåˆ¶ä¸ä½¿ç”¨
        return await super()._fetch_detail(ctx, url, None)

    async def _get_url_content_length(self, url: str) -> int | None:
        """è·å–URLçš„Content-Lengthï¼ˆæ–‡ä»¶å¤§å°ï¼‰

        å…ˆå°è¯•HEADè¯·æ±‚ï¼Œå¦‚æœè¿”å›405åˆ™æ”¹ç”¨GETè¯·æ±‚å¹¶ç«‹å³å…³é—­è¿æ¥
        åŒ…å«é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡é‡è¯•ï¼‰
        """
        max_retries = 3
        retry_delays = [0.5, 1.0, 1.5]

        for attempt in range(max_retries):
            try:
                # å…ˆå°è¯•HEADè¯·æ±‚
                async with manager.computed.async_client.request("HEAD", url, timeout=10) as resp:
                    if resp.status == 200:
                        content_length = resp.headers.get("content-length")
                        if content_length:
                            return int(content_length)
                    elif resp.status == 405:
                        # 405 Method Not Allowedï¼Œæ”¹ç”¨GETè¯·æ±‚
                        break

            except Exception:
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delays[attempt])
                    continue
                return None

        # ä½¿ç”¨GETè¯·æ±‚è·å–æ–‡ä»¶å¤§å°
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨stream=Trueåªè¯»å–å“åº”å¤´è€Œä¸ä¸‹è½½å†…å®¹
                async with manager.computed.async_client.request("GET", url, timeout=10) as resp:
                    if resp.status == 200:
                        content_length = resp.headers.get("content-length")
                        if content_length:
                            return int(content_length)
                    return None
            except Exception:
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delays[attempt])
                    continue
                return None

        return None

    @override
    async def post_process(self, ctx, res):
        if not res.number:
            res.number = ctx.input.number
        # å¯¹äºVRè§†é¢‘æˆ–SODå·¥ä½œå®¤ï¼Œç›´æ¥ä½¿ç”¨ps.jpgè€Œä¸è¿›è¡Œè£å‰ª
        # SODç³»åˆ—é€šå¸¸é‡‡ç”¨ç‰¹æ®Šçš„å®½é«˜æ¯”ï¼Œæ— æ³•é€šè¿‡è£å‰ªè·å¾—æœ€ä½³æ•ˆæœ
        is_sod_studio = "SOD" in (res.studio or "")
        use_direct_download = "VR" in res.title or is_sod_studio

        # è°ƒè¯•ï¼šè¾“å‡ºstudioå­—æ®µå’ŒSODæ£€æµ‹ç»“æœ
        signal.add_log(
            f"ğŸ“Š [è°ƒè¯•] è§†é¢‘ {res.number} studio: '{res.studio}', is_sod: {is_sod_studio}, "
            f"poster: {bool(res.poster)}, thumb: {bool(res.thumb)}"
        )

        if is_sod_studio and res.poster and res.thumb:
            # å¯¹SODå·¥ä½œå®¤ï¼Œæ¯”è¾ƒps.jpgå’Œpl.jpgçš„å¤§å°
            # å¦‚æœps.jpgåˆ†è¾¨ç‡æ˜æ˜¾ä½äºpl.jpgï¼Œåˆ™ä½¿ç”¨è£å‰ªåçš„posterè€Œä¸æ˜¯ç›´æ¥ä¸‹è½½
            ps_url = res.poster  # ps.jpg
            pl_url = res.thumb  # pl.jpg
            try:
                # è·å–ä¸¤ä¸ªæ–‡ä»¶çš„å¤§å°
                ps_size = await self._get_url_content_length(ps_url)
                pl_size = await self._get_url_content_length(pl_url)

                if ps_size and pl_size:
                    # å¦‚æœps.jpgå¤§å°ä¸è¶³pl.jpgçš„50%ï¼Œåˆ™è®¤ä¸ºåˆ†è¾¨ç‡å¤ªä½ï¼Œæ”¹ç”¨è£å‰ªç‰ˆæœ¬
                    if ps_size < pl_size * 0.5:
                        signal.add_log(
                            f"SODå·¥ä½œå®¤ps.jpgåˆ†è¾¨ç‡è¿‡ä½({ps_size}B) vs pl.jpg({pl_size}B)ï¼Œ"
                            f"å°†ä½¿ç”¨è£å‰ªåçš„å›¾ç‰‡è€Œä¸æ˜¯ç›´æ¥ä¸‹è½½"
                        )
                        use_direct_download = "VR" in res.title
                    else:
                        signal.add_log(
                            f"æ£€æµ‹åˆ°SODå·¥ä½œå®¤: {res.studio}ï¼Œps.jpgåˆ†è¾¨ç‡å……è¶³({ps_size}B)ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹å›¾ç‰‡ä¸è¿›è¡Œè£å‰ª"
                        )
                else:
                    signal.add_log(f"æ£€æµ‹åˆ°SODå·¥ä½œå®¤: {res.studio}ï¼Œæ— æ³•è·å–å›¾ç‰‡å¤§å°ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹å›¾ç‰‡ä¸è¿›è¡Œè£å‰ª")
            except Exception as e:
                signal.add_log(f"SODå·¥ä½œå®¤å›¾ç‰‡å¤§å°æ¯”è¾ƒå¤±è´¥: {e}ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹å›¾ç‰‡ä¸è¿›è¡Œè£å‰ª")

        res.image_download = use_direct_download
        res.originaltitle = res.title
        res.originalplot = res.outline
        # check aws image
        if res.thumb and "pics.dmm.co.jp" in res.thumb:
            aws_urls = [
                res.thumb.replace("pics.dmm.co.jp", "awsimgsrc.dmm.co.jp/pics_dig").replace("/adult/", "/"),
                f"https://awsimgsrc.dmm.co.jp/pics_dig/digital/video/{ctx.number_00}/{ctx.number_00}pl.jpg",
                f"https://awsimgsrc.dmm.co.jp/pics_dig/digital/video/{ctx.number_no_00}/{ctx.number_no_00}pl.jpg",
            ]
            for aws_url in aws_urls:
                if await check_url(aws_url):
                    ctx.debug(f"use aws image: {aws_url}")
                    res.thumb = aws_url
                    break
        res.poster = res.thumb.replace("pl.jpg", "ps.jpg")
        if not res.publisher:
            res.publisher = res.studio
        if len(res.release) >= 4:
            res.year = res.release[:4]
        return res

    @override
    async def _parse_detail_page(self, ctx, html: Selector, detail_url: str) -> CrawlerData | None:
        raise NotImplementedError
