import asyncio
import html as html_utils
import json
import re
from collections import defaultdict
from collections.abc import Sequence
from typing import override
from urllib.parse import urljoin

from parsel import Selector
from patchright._impl._api_structures import SetCookieParam
from patchright.async_api import Browser

from mdcx.base.web import check_url
from mdcx.config.manager import manager
from mdcx.config.models import Website
from mdcx.models.types import CrawlerInput
from mdcx.signals import signal
from mdcx.utils.dataclass import update_valid
from mdcx.utils.gather_group import GatherGroup
from mdcx.web_async import AsyncWebClient

from ..base import (
    Context,
    CralwerException,
    CrawlerData,
    DetailPageParser,
    GenericBaseCrawler,
    is_valid,
)
from .parsers import Category, DigitalParser, MonoParser, RentalParser, parse_category
from .tv import DmmTvResponse, FanzaResp, dmm_tv_com_payload, fanza_tv_payload


class DMMContext(Context):
    number_00: str | None = None
    number_no_00: str | None = None


class DmmCrawler(GenericBaseCrawler[DMMContext]):
    mono = MonoParser()
    digital = DigitalParser()
    rental = RentalParser()

    def __init__(self, client: AsyncWebClient, base_url: str = "", browser: Browser | None = None):
        super().__init__(client, base_url, browser)

    async def _http_request_with_retry(self, method: str, url: str, **kwargs):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„ HTTP è¯·æ±‚

        Args:
            method: HTTP æ–¹æ³• ('GET', 'POST', 'HEAD')
            url: è¯·æ±‚ URL
            **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°

        Returns:
            (response, error) å…ƒç»„
        """
        max_retries = manager.config.retry  # ä»é…ç½®è·å–é‡è¯•æ¬¡æ•°

        last_error = None

        for attempt in range(max_retries + 1):
            try:
                if method.upper() == "POST":
                    if "json_data" in kwargs:
                        response, error = await self.async_client.post_json(url, **kwargs)
                    else:
                        response, error = await self.async_client.post_text(url, **kwargs)
                elif method.upper() == "GET":
                    response, error = await self.async_client.get_text(url, **kwargs)
                elif method.upper() == "HEAD":
                    response, error = await self.async_client.request("HEAD", url, **kwargs)
                else:
                    response, error = await self.async_client.request(method, url, **kwargs)

                # å¦‚æœè¯·æ±‚æˆåŠŸï¼Œç›´æ¥è¿”å›
                if response is not None:
                    return response, error

                # è®°å½•å¤±è´¥ä¿¡æ¯
                last_error = error

            except Exception as e:
                last_error = str(e)

            # é‡è¯•å‰ç­‰å¾…ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            if attempt < max_retries:
                wait_time = min(2**attempt, 10)  # æœ€å¤šç­‰å¾…10ç§’
                await asyncio.sleep(wait_time)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        return None, f"è¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {last_error}"

    @classmethod
    @override
    def site(cls) -> Website:
        return Website.DMM

    @classmethod
    @override
    def base_url_(cls) -> str:
        # DMM ä¸æ”¯æŒè‡ªå®šä¹‰ URL
        return ""

    @override
    def new_context(self, input: CrawlerInput) -> DMMContext:
        return DMMContext(input=input)

    @override
    def _get_cookies(self, ctx) -> dict[str, str] | None:
        return {"age_check_done": "1"}

    @override
    def _get_cookies_browser(self, ctx: DMMContext) -> Sequence[SetCookieParam] | None:
        return [
            SetCookieParam(name="age_check_done", value="1", domain=".dmm.co.jp", path="/"),
            SetCookieParam(name="age_check_done", value="1", domain=".dmm.com", path="/"),
        ]

    @override
    async def _generate_search_url(self, ctx) -> list[str] | None:
        number = ctx.input.number.lower()

        if x := re.findall(r"[A-Za-z]+-?(\d+)", number):
            digits = x[0]
            if len(digits) >= 5 and digits.startswith("00"):
                number = number.replace(digits, digits[2:])
            elif len(digits) == 4:
                number = number.replace("-", "0")  # https://github.com/sqzw-x/mdcx/issues/393

        # æœç´¢ç»“æœå¤šï¼Œä½†snis-027æ²¡ç»“æœ
        number_00 = number.replace("-", "00")
        # æœç´¢ç»“æœå°‘
        number_no_00 = number.replace("-", "")
        ctx.number_00 = number_00
        ctx.number_no_00 = number_no_00

        return [
            f"https://www.dmm.co.jp/search/=/searchstr={number_00}/sort=ranking/",
            f"https://www.dmm.co.jp/search/=/searchstr={number_no_00}/sort=ranking/",
            f"https://www.dmm.com/search/=/searchstr={number_no_00}/sort=ranking/",  # å†™çœŸ
        ]

    @override
    async def _parse_search_page(self, ctx, html, search_url) -> list[str] | None:
        if "404 Not Found" in html.css("span.d-txten::text").get(""):
            raise CralwerException("404! é¡µé¢åœ°å€é”™è¯¯ï¼")

        # \"detailUrl\":\"https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=ssni00103/?i3_ord=1\u0026i3_ref=search"
        url_list = set(html.re(r'detailUrl\\":\\"(.*?)\\"'))
        if not url_list:
            ctx.debug(f"æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœ: {ctx.input.number} {search_url=}")
            return None

        number_parts: re.Match[str] | None = re.search(r"(\d*[a-z]+)?-?(\d+)", ctx.input.number.lower())
        if not number_parts:
            ctx.debug(f"æ— æ³•ä»ç•ªå· {ctx.input.number} æå–å‰ç¼€å’Œæ•°å­—")
            return None
        prefix = number_parts.group(1)
        digits = number_parts.group(2)
        n1 = f"{prefix}{digits:0>5}"
        n2 = f"{prefix}{digits}"

        res = []
        for u in url_list:
            # https://tv.dmm.co.jp/list/?content=mide00726&i3_ref=search&i3_ord=1
            # https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=mide00726/?i3_ref=search&i3_ord=2
            # https://www.dmm.com/mono/dvd/-/detail/=/cid=n_709mmrak089sp/?i3_ref=search&i3_ord=1
            if re.search(rf"[^a-z]{n1}[^0-9]", u) or re.search(rf"[^a-z]{n2}[^0-9]", u):
                res.append(u.encode("utf-8").decode("unicode_escape"))

        return res

    @classmethod
    def _get_parser(cls, category: Category):
        match category:
            case Category.PRIME | Category.MONTHLY | Category.MONO:
                return cls.mono
            case Category.DIGITAL:
                return cls.digital
            case Category.RENTAL:
                return cls.rental

    @override
    async def _detail(self, ctx: DMMContext, detail_urls: list[str]) -> CrawlerData | None:
        d = defaultdict(list)
        for url in detail_urls:
            category = parse_category(url)
            d[category].append(url)

        # è®¾ç½® GatherGroup çš„æ•´ä½“è¶…æ—¶æ—¶é—´ï¼Œç»™å•ä¸ªè¯·æ±‚æ›´å¤šæ—¶é—´
        # å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å•ä¸ªè¯·æ±‚ä¸­å®ç°äº†é‡è¯•æœºåˆ¶
        total_timeout = manager.config.timeout * (manager.config.retry + 1) * 2  # ç»™è¶³å¤Ÿçš„æ—¶é—´

        async with GatherGroup[CrawlerData](timeout=total_timeout) as group:
            for url in d[Category.FANZA_TV]:
                group.add(self.fetch_fanza_tv(ctx, url))
            for url in d[Category.DMM_TV]:
                group.add(self.fetch_dmm_tv(ctx, url))

            for category in (
                Category.DIGITAL,
                Category.MONO,
                Category.RENTAL,
                Category.PRIME,
                Category.MONTHLY,
            ):  # ä¼˜å…ˆçº§
                parser = self._get_parser(category)
                if parser is None:
                    continue
                for u in sorted(d[category]):
                    group.add(self.fetch_and_parse(ctx, u, parser))

        res = None
        best_trailer = ""
        for r in group.results[::-1]:
            if isinstance(r, Exception):  # é¢„è®¡åªä¼šè¿”å›ç©ºå€¼, ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
                ctx.debug(f"é¢„æ–™ä¹‹å¤–çš„å¼‚å¸¸: {r}")
                continue

            if is_valid(r.trailer):
                candidate_trailer = str(r.trailer)
                if self._is_hls_playlist_trailer(candidate_trailer):
                    ctx.debug(f"è·³è¿‡ m3u8 é¢„å‘Šç‰‡å€™é€‰: url={candidate_trailer}")
                    continue
                candidate_rank = self._trailer_quality_rank(candidate_trailer)
                source_hint = f" external_id={r.external_id}" if is_valid(r.external_id) else ""
                ctx.debug(f"trailer å€™é€‰: rank={candidate_rank}{source_hint} url={candidate_trailer}")

                previous_best = best_trailer
                best_trailer = self._pick_higher_quality_trailer(best_trailer, candidate_trailer)
                if best_trailer != previous_best:
                    if previous_best:
                        prev_rank = self._trailer_quality_rank(previous_best)
                        ctx.debug(
                            f"trailer æœ€ä¼˜æ›´æ–°: rank {prev_rank} -> {candidate_rank}; "
                            f"old={previous_best}; new={best_trailer}"
                        )
                    else:
                        ctx.debug(f"trailer åˆå§‹æœ€ä¼˜: rank={candidate_rank}; url={best_trailer}")

            if res is None:
                res = r
            else:
                res = update_valid(res, r, is_valid)

        if res is not None and best_trailer:
            if not is_valid(res.trailer):
                ctx.debug(f"trailer æœ€ç»ˆé‡‡ç”¨æœ€ä¼˜å€™é€‰(è¡¥å…¨ç©ºå€¼): {best_trailer}")
            elif str(res.trailer) != best_trailer:
                ctx.debug(f"trailer æœ€ç»ˆæ”¹å†™ä¸ºæ›´é«˜è´¨é‡: old={res.trailer}; new={best_trailer}")
            res.trailer = best_trailer
        elif res is not None and is_valid(res.trailer) and self._is_hls_playlist_trailer(str(res.trailer)):
            ctx.debug(f"trailer æœ€ç»ˆæ¸…ç©º m3u8 é“¾æ¥: old={res.trailer}")
            res.trailer = ""

        return res

    @staticmethod
    def _trailer_quality_rank(trailer_url: str) -> int:
        quality_levels = {
            "sm": 1,
            "dm": 2,
            "dmb": 3,
            "mmb": 4,
            "hmb": 5,
            "mhb": 6,
            "hhb": 7,
            "4k": 8,
        }
        alias = {
            "mmbs": "mmb",
            "hmbs": "hmb",
            "mhbs": "mhb",
            "hhbs": "hhb",
            "4ks": "4k",
        }

        if matched := re.search(
            r"_(sm|dm|dmb|mmb|hmb|mhb|hhb|4k|mmbs|hmbs|mhbs|hhbs|4ks)_[a-z]\.mp4$",
            trailer_url,
            flags=re.IGNORECASE,
        ):
            quality = alias.get(matched.group(1).lower(), matched.group(1).lower())
            return quality_levels.get(quality, 0)

        if matched := re.search(
            r"(sm|dm|dmb|mmb|hmb|mhb|hhb|4k|mmbs|hmbs|mhbs|hhbs|4ks)\.mp4$",
            trailer_url,
            flags=re.IGNORECASE,
        ):
            quality = alias.get(matched.group(1).lower(), matched.group(1).lower())
            return quality_levels.get(quality, 0)

        return 0

    @staticmethod
    def _is_hls_playlist_trailer(trailer_url: str) -> bool:
        trailer_url = str(trailer_url or "").lower()
        return ".m3u8" in trailer_url

    @classmethod
    def _pick_higher_quality_trailer(cls, current_url: str, candidate_url: str) -> str:
        if not current_url:
            return candidate_url

        current_rank = cls._trailer_quality_rank(current_url)
        candidate_rank = cls._trailer_quality_rank(candidate_url)

        if candidate_rank > current_rank:
            return candidate_url

        return current_url

    @staticmethod
    def _is_valid_dmm_cid(cid: str) -> bool:
        return bool(
            cid
            and "." not in cid
            and re.search(r"[a-z]", cid, flags=re.IGNORECASE)
            and re.search(r"\d", cid)
        )

    @classmethod
    def _build_pv_trailer_from_thumbnail(cls, thumbnail_url: str) -> str:
        thumbnail_url = cls._with_https(str(thumbnail_url or "").strip())
        matched = re.search(
            r"https?://pics\.litevideo\.dmm\.co\.jp/pv/([^/?#]+)/([^/?#]+)\.jpg(?:[?#].*)?$",
            thumbnail_url,
            flags=re.IGNORECASE,
        )
        if not matched:
            return ""
        token, stem = matched.groups()
        if not cls._is_valid_dmm_cid(stem):
            return ""
        return f"https://cc3001.dmm.co.jp/pv/{token}/{stem}mhb.mp4"

    @classmethod
    def _build_freepv_trailer_from_cid(cls, cid: str, quality_suffix: str = "_sm_w") -> str:
        cid = str(cid or "").strip().lower()
        if not cls._is_valid_dmm_cid(cid):
            return ""
        return f"https://cc3001.dmm.co.jp/litevideo/freepv/{cid[0]}/{cid[:3]}/{cid}/{cid}{quality_suffix}.mp4"

    @staticmethod
    def _extract_litevideo_player_url(detail_html: str) -> str:
        if not detail_html:
            return ""
        if not (matched := re.search(r'<iframe[^>]+src="([^"]+digitalapi[^"]+)"', detail_html, flags=re.IGNORECASE)):
            return ""
        return DmmCrawler._with_https(html_utils.unescape(matched.group(1)))

    @classmethod
    def _extract_litevideo_trailer_candidates(cls, player_html: str) -> list[str]:
        if not player_html:
            return []
        trailers: list[str] = []
        for source in re.findall(
            r'"src":"(\\/\\/cc3001\.dmm\.co\.jp\\/pv\\/[^\"]+?\.mp4)"',
            player_html,
            flags=re.IGNORECASE,
        ):
            trailer_url = cls._with_https(source.replace("\\/", "/"))
            if trailer_url and trailer_url not in trailers:
                trailers.append(trailer_url)
        return trailers

    async def _fetch_litevideo_trailer_candidates(self, ctx: Context, content_cid: str) -> list[str]:
        detail_url = f"https://www.dmm.co.jp/litevideo/-/detail/=/cid={content_cid}/"
        detail_html, error = await self._http_request_with_retry("GET", detail_url)
        if detail_html is None:
            ctx.debug(f"litevideo è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {content_cid=} {error=}")
            return []

        player_url = self._extract_litevideo_player_url(detail_html)
        if not player_url:
            ctx.debug(f"litevideo è¯¦æƒ…é¡µæœªæ‰¾åˆ°æ’­æ”¾å™¨ iframe: {content_cid=}")
            return []

        player_html, error = await self._http_request_with_retry("GET", player_url)
        if player_html is None:
            ctx.debug(f"litevideo æ’­æ”¾å™¨é¡µè¯·æ±‚å¤±è´¥: {content_cid=} {error=}")
            return []

        return self._extract_litevideo_trailer_candidates(player_html)

    @classmethod
    def _build_fanza_trailer_url(
        cls,
        sample_movie_url: str,
        sample_movie_thumbnail: str = "",
        fallback_cid: str = "",
    ) -> str:
        raw_url = cls._with_https(str(sample_movie_url or "").strip())
        if not raw_url:
            return ""

        if re.search(r"\.mp4(?:[?#].*)?$", raw_url, flags=re.IGNORECASE):
            return raw_url

        trailer_url = raw_url.replace("hlsvideo", "litevideo")

        if "/pv/" in trailer_url and "playlist.m3u8" in trailer_url:
            return ""

        cid_match = re.search(r"/([^/]+)/playlist\.m3u8", trailer_url)
        if cid_match:
            cid_from_url = cid_match.group(1)
            return trailer_url.replace("playlist.m3u8", cid_from_url + "_sm_w.mp4")
        return ""

    @classmethod
    def _build_fanza_fallback_candidates(cls, sample_movie_thumbnail: str, fallback_cid: str) -> list[str]:
        candidates: list[str] = []

        for suffix in ("_4k_w", "_hhb_w", "_mhb_w", "_hmb_w", "_mmb_w", "_dmb_w", "_dm_w", "_sm_w"):
            trailer = cls._build_freepv_trailer_from_cid(fallback_cid, quality_suffix=suffix)
            if trailer and trailer not in candidates:
                candidates.append(trailer)

        if trailer_from_thumb := cls._build_pv_trailer_from_thumbnail(sample_movie_thumbnail):
            if trailer_from_thumb not in candidates:
                candidates.append(trailer_from_thumb)

        return candidates

    async def _validate_trailer_url(self, ctx: Context, trailer_url: str) -> str:
        trailer_url = self._with_https(str(trailer_url or "").strip())
        if not trailer_url:
            return ""

        cookies = self._get_cookies(ctx)
        checks: list[tuple[str, dict[str, str] | None]] = [
            ("HEAD", None),
            ("GET", {"Range": "bytes=0-0"}),
        ]

        for method, headers in checks:
            response, error = await self.async_client.request(method, trailer_url, headers=headers, cookies=cookies)
            if response is None:
                ctx.debug(f"trailer æ ¡éªŒå¤±è´¥: {method} {trailer_url} {error=}")
                continue

            if response.status_code not in (200, 206):
                continue

            content_type = str(response.headers.get("Content-Type") or "").lower()
            if "text/html" in content_type or "application/xml" in content_type:
                continue
            if content_type and "video" not in content_type and "octet-stream" not in content_type:
                continue

            return str(response.url)

        return ""

    async def _pick_best_valid_trailer(self, ctx: Context, candidates: list[str]) -> str:
        best_trailer = ""
        for trailer_url in dict.fromkeys(candidates):
            validated = await self._validate_trailer_url(ctx, trailer_url)
            if not validated:
                continue
            best_trailer = self._pick_higher_quality_trailer(best_trailer, validated)
        return best_trailer

    @classmethod
    def _pick_best_unvalidated_trailer(cls, current_url: str, candidates: list[str]) -> str:
        best_trailer = current_url
        for trailer_url in dict.fromkeys(candidates):
            trailer_url = cls._with_https(str(trailer_url or "").strip())
            if not trailer_url:
                continue
            if cls._is_hls_playlist_trailer(trailer_url):
                continue
            best_trailer = cls._pick_higher_quality_trailer(best_trailer, trailer_url)
        return best_trailer

    async def fetch_fanza_tv(self, ctx: Context, detail_url: str) -> CrawlerData:
        cid_match = re.search(r"content=([^&/]+)", detail_url)
        if not cid_match:
            ctx.debug(f"æ— æ³•ä» DMM TV URL æå– cid: {detail_url}")
            return CrawlerData()
        content_cid = cid_match.group(1).lower()

        # ä½¿ç”¨å¸¦é‡è¯•çš„ HTTP è¯·æ±‚
        response, error = await self._http_request_with_retry(
            "POST", "https://api.tv.dmm.co.jp/graphql", json_data=fanza_tv_payload(content_cid)
        )
        if response is None:
            ctx.debug(f"Fanza TV API è¯·æ±‚å¤±è´¥: {content_cid=} {error=}")
            return CrawlerData()
        try:
            resp = FanzaResp.model_validate(response)
            data = resp.data.fanzaTvPlus.content
        except Exception as e:
            ctx.debug(f"Fanza TV API å“åº”è§£æå¤±è´¥: {e}")
            return CrawlerData()

        extrafanart = []
        for sample_pic in data.samplePictures:
            if sample_pic.imageLarge:
                extrafanart.append(sample_pic.imageLarge)

        trailer = self._build_fanza_trailer_url(
            data.sampleMovie.url,
            sample_movie_thumbnail=data.sampleMovie.thumbnail,
            fallback_cid=content_cid,
        )
        trailer = self._pick_best_unvalidated_trailer("", [trailer] if trailer else [])
        if trailer:
            signal.add_log(
                f"ğŸ¬ DMMé¢„å‘Šç‰‡[è¯¦æƒ…æºç›´å–]: cid={content_cid} rank={self._trailer_quality_rank(trailer)} {trailer}"
            )

        should_try_litevideo = not trailer or self._trailer_quality_rank(trailer) < self._trailer_quality_rank("xhhb.mp4")
        if should_try_litevideo:
            litevideo_candidates = await self._fetch_litevideo_trailer_candidates(ctx, content_cid)
            if litevideo_candidates:
                ctx.debug(f"litevideo ç›´è¿é¢„å‘Šç‰‡å€™é€‰æ•°: {len(litevideo_candidates)} {content_cid=}")
                signal.add_log(f"ğŸ¬ DMMé¢„å‘Šç‰‡[litevideoå€™é€‰]: cid={content_cid} count={len(litevideo_candidates)}")
                best_litevideo = self._pick_best_unvalidated_trailer("", litevideo_candidates)
                if best_litevideo:
                    signal.add_log(
                        f"ğŸ¬ DMMé¢„å‘Šç‰‡[litevideoæœ€ä¼˜]: cid={content_cid} rank={self._trailer_quality_rank(best_litevideo)} {best_litevideo}"
                    )
                trailer = self._pick_higher_quality_trailer(trailer, best_litevideo)

        if not trailer:
            fallback_candidates = self._build_fanza_fallback_candidates(
                sample_movie_thumbnail=data.sampleMovie.thumbnail,
                fallback_cid=content_cid,
            )
            signal.add_log(f"ğŸ¬ DMMé¢„å‘Šç‰‡[å…œåº•æ ¡éªŒ]: cid={content_cid} count={len(fallback_candidates)}")
            trailer = await self._pick_best_valid_trailer(ctx, fallback_candidates)
            if trailer:
                signal.add_log(
                    f"ğŸ¬ DMMé¢„å‘Šç‰‡[å…œåº•å‘½ä¸­]: cid={content_cid} rank={self._trailer_quality_rank(trailer)} {trailer}"
                )

        if trailer:
            signal.add_log(f"ğŸ¬ DMMé¢„å‘Šç‰‡[æœ€ç»ˆ]: cid={content_cid} rank={self._trailer_quality_rank(trailer)} {trailer}")
        else:
            signal.add_log(f"ğŸŸ  DMMé¢„å‘Šç‰‡[æœ€ç»ˆ]: cid={content_cid} æœªè·å–åˆ°å¯ç”¨é“¾æ¥")

        return CrawlerData(
            title=data.title,
            outline=data.description,
            release=data.startDeliveryAt,  # 2025-05-17T20:00:00Z
            tags=[genre.name for genre in data.genres],
            runtime=str(int(data.playInfo.duration / 60)),
            actors=[a.name for a in data.actresses],
            poster=data.packageImage,
            thumb=data.packageLargeImage,
            score=str(data.reviewSummary.averagePoint),
            series=data.series.name,
            directors=[d.name for d in data.directors],
            studio=data.maker.name,
            publisher=data.label.name,
            extrafanart=extrafanart,
            trailer=trailer,
            external_id=detail_url,
        )

    async def fetch_dmm_tv(self, ctx: Context, detail_url: str) -> CrawlerData:
        season_id = re.search(r"seasonId=(\d+)", detail_url)
        if not season_id:
            ctx.debug(f"æ— æ³•ä» DMM TV URL æå– seasonId: {detail_url}")
            return CrawlerData()
        season_id = season_id.group(1)

        # ä½¿ç”¨å¸¦é‡è¯•çš„ HTTP è¯·æ±‚
        response, error = await self._http_request_with_retry(
            "POST", "https://api.tv.dmm.com/graphql", json_data=dmm_tv_com_payload(season_id)
        )
        if response is None:
            ctx.debug(f"DMM TV API è¯·æ±‚å¤±è´¥: {season_id=} {error=}")
            return CrawlerData()
        try:
            resp = DmmTvResponse.model_validate(response)
            data = resp.data.video
        except Exception as e:
            ctx.debug(f"DMM TV API å“åº”è§£æå¤±è´¥: {e}")
            return CrawlerData()

        studio = ""
        if r := [item.staffName for item in data.staffs if item.roleName in ["åˆ¶ä½œãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³", "åˆ¶ä½œ", "åˆ¶ä½œè‘—ä½œ"]]:
            studio = r[0]

        return CrawlerData(
            title=data.titleName,
            outline=data.description,
            actors=[item.actorName for item in data.casts],
            poster=data.packageImage,
            thumb=data.keyVisualImage,
            tags=[item.name for item in data.genres],
            release=data.startPublicAt,  # 2025-05-17T20:00:00Z
            year=str(data.productionYear),
            score=str(data.reviewSummary.averagePoint),
            directors=[item.staffName for item in data.staffs if item.roleName == "ç›£ç£"],
            studio=studio,
            publisher=studio,
            external_id=detail_url,
        )

    @staticmethod
    def _with_https(url: str) -> str:
        if url.startswith("//"):
            return "https:" + url
        return url

    @staticmethod
    def _extract_mono_trailer_from_ga_event(detail_html: str) -> str:
        if not (matched := re.search(r"gaEventVideoStart\('([^']+)'", detail_html)):
            return ""

        payload = html_utils.unescape(matched.group(1))
        try:
            data = json.loads(payload)
        except Exception:
            return ""

        trailer_url = str(data.get("video_url") or "").replace("\\/", "/")
        return DmmCrawler._with_https(trailer_url)

    @staticmethod
    def _extract_mono_ajax_movie_path(detail_html: str) -> str:
        if matched := re.search(r'data-video-url="([^"]+)"', detail_html):
            return html_utils.unescape(matched.group(1))
        if matched := re.search(r"sampleVideoRePlay\('([^']+)'\)", detail_html):
            return html_utils.unescape(matched.group(1))
        return ""

    @staticmethod
    def _extract_player_iframe_url(ajax_movie_html: str) -> str:
        if matched := re.search(r'src="([^"]+)"', ajax_movie_html):
            return DmmCrawler._with_https(html_utils.unescape(matched.group(1)))
        return ""

    @staticmethod
    def _extract_mono_trailer_from_player(player_html: str) -> str:
        if not (matched := re.search(r"const\s+args\s*=\s*(\{.*?\});", player_html, flags=re.DOTALL)):
            return ""

        try:
            args = json.loads(matched.group(1))
        except Exception:
            return ""

        bitrates = args.get("bitrates") or []
        for item in bitrates:
            if trailer_url := str(item.get("src") or ""):
                return DmmCrawler._with_https(trailer_url)

        return DmmCrawler._with_https(str(args.get("src") or ""))

    async def _fetch_mono_trailer(self, ctx: DMMContext, detail_url: str, detail_html: str) -> str:
        trailer_url = self._extract_mono_trailer_from_ga_event(detail_html)
        if trailer_url:
            return trailer_url

        ajax_movie_path = self._extract_mono_ajax_movie_path(detail_html)
        if not ajax_movie_path:
            return ""

        ajax_movie_url = urljoin(detail_url, ajax_movie_path)
        ajax_movie_html, error = await super()._fetch_detail(ctx, ajax_movie_url, False)
        if ajax_movie_html is None:
            ctx.debug(f"mono ajax-movie è¯·æ±‚å¤±è´¥: {ajax_movie_url=} {error=}")
            return ""

        player_iframe_url = self._extract_player_iframe_url(ajax_movie_html)
        if not player_iframe_url:
            return ""

        player_html, error = await super()._fetch_detail(ctx, player_iframe_url, False)
        if player_html is None:
            ctx.debug(f"mono player è¯·æ±‚å¤±è´¥: {player_iframe_url=} {error=}")
            return ""

        return self._extract_mono_trailer_from_player(player_html)

    async def fetch_and_parse(self, ctx: DMMContext, detail_url: str, parser: DetailPageParser) -> CrawlerData:
        html, error = await self._fetch_detail(ctx, detail_url)
        if html is None:
            ctx.debug(f"è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {error=}")
            return CrawlerData()
        ctx.debug(f"è¯¦æƒ…é¡µè¯·æ±‚æˆåŠŸ: {detail_url=}")

        parsed = await parser.parse(ctx, Selector(html), external_id=detail_url)

        if parse_category(detail_url) == Category.MONO and not is_valid(parsed.trailer):
            trailer_url = await self._fetch_mono_trailer(ctx, detail_url, html)
            if trailer_url:
                parsed.trailer = trailer_url

        return parsed

    @override
    async def _fetch_detail(self, ctx: DMMContext, url: str, use_browser=None) -> tuple[str | None, str]:
        if parse_category(url) not in (Category.DIGITAL):
            return await super()._fetch_detail(ctx, url, False)  # å¯¹äºç¡®å®šä¸éœ€è¦æµè§ˆå™¨çš„, å¼ºåˆ¶ä¸ä½¿ç”¨
        return await super()._fetch_detail(ctx, url, None)

    async def _get_url_content_length(self, url: str) -> int | None:
        """è·å–URLçš„Content-Lengthï¼ˆæ–‡ä»¶å¤§å°ï¼‰

        å…ˆå°è¯•HEADè¯·æ±‚ï¼Œå¦‚æœè¿”å›405åˆ™æ”¹ç”¨GETè¯·æ±‚
        åŒ…å«é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡é‡è¯•ï¼‰
        """
        max_retries = 3
        retry_delays = [0.5, 1.0, 1.5]

        for attempt in range(max_retries):
            try:
                # å…ˆå°è¯•HEADè¯·æ±‚
                response, error = await manager.computed.async_client.request("HEAD", url)

                if response is not None:
                    if response.status_code == 200:
                        content_length = response.headers.get("Content-Length")
                        if content_length:
                            signal.add_log(f"HEADè·å–æ–‡ä»¶å¤§å°æˆåŠŸ: {url} -> {content_length}B")
                            return int(content_length)
                    elif response.status_code == 405:
                        # 405 Method Not Allowedï¼Œæ”¹ç”¨GETè¯·æ±‚
                        signal.add_log(f"HEADè¯·æ±‚è¿”å›405ï¼Œå°†åˆ‡æ¢ä¸ºGETè¯·æ±‚: {url}")
                        break
                    else:
                        signal.add_log(f"HEADè¯·æ±‚è¿”å›{response.status_code}: {url}")
                elif error:
                    signal.add_log(f"HEADè¯·æ±‚å¼‚å¸¸(å°è¯•{attempt + 1}/{max_retries}): {url} -> {error}")

            except Exception as e:
                signal.add_log(f"HEADè¯·æ±‚å¼‚å¸¸(å°è¯•{attempt + 1}/{max_retries}): {url} -> {e}")

            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delays[attempt])

        # ä½¿ç”¨GETè¯·æ±‚è·å–æ–‡ä»¶å¤§å°
        for attempt in range(max_retries):
            try:
                response, error = await manager.computed.async_client.request("GET", url)

                if response is not None:
                    if response.status_code == 200:
                        content_length = response.headers.get("Content-Length")
                        if content_length:
                            signal.add_log(f"GETè·å–æ–‡ä»¶å¤§å°æˆåŠŸ: {url} -> {content_length}B")
                            return int(content_length)
                        else:
                            signal.add_log(f"GETè¯·æ±‚æˆåŠŸä½†æ— Content-Lengthå¤´: {url}")
                    else:
                        signal.add_log(f"GETè¯·æ±‚è¿”å›{response.status_code}: {url}")
                elif error:
                    signal.add_log(f"GETè¯·æ±‚å¼‚å¸¸(å°è¯•{attempt + 1}/{max_retries}): {url} -> {error}")

                return None

            except Exception as e:
                signal.add_log(f"GETè¯·æ±‚å¼‚å¸¸(å°è¯•{attempt + 1}/{max_retries}): {url} -> {e}")

            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delays[attempt])

        return None

    @override
    async def post_process(self, ctx, res):
        if not res.number:
            res.number = ctx.input.number
        # å¯¹äºVRè§†é¢‘æˆ–SODå·¥ä½œå®¤ï¼Œç›´æ¥ä½¿ç”¨ps.jpgè€Œä¸è¿›è¡Œè£å‰ª
        # SODç³»åˆ—é€šå¸¸é‡‡ç”¨ç‰¹æ®Šçš„å®½é«˜æ¯”ï¼Œæ— æ³•é€šè¿‡è£å‰ªè·å¾—æœ€ä½³æ•ˆæœ
        is_sod_studio = "SOD" in (res.studio or "")
        use_direct_download = "VR" in res.title or is_sod_studio

        res.image_download = use_direct_download
        res.originaltitle = res.title
        res.originalplot = res.outline
        # check aws image
        if res.thumb and "pics.dmm.co.jp" in res.thumb:
            aws_urls = [
                res.thumb.replace("pics.dmm.co.jp", "awsimgsrc.dmm.co.jp/pics_dig").replace("/adult/", "/"),
                f"https://awsimgsrc.dmm.co.jp/pics_dig/digital/video/{ctx.number_00}/{ctx.number_00}pl.jpg",
                f"https://awsimgsrc.dmm.co.jp/pics_dig/digital/video/{ctx.number_no_00}/{ctx.number_no_00}pl.jpg",
            ]
            for aws_url in aws_urls:
                if await check_url(aws_url):
                    signal.add_log(f"DMM ä½¿ç”¨ AWS é«˜æ¸…å›¾: {aws_url}")
                    res.thumb = aws_url
                    break
        res.poster = res.thumb.replace("pl.jpg", "ps.jpg")

        # å¯¹SODå·¥ä½œå®¤è¿›è¡Œå›¾ç‰‡å¤§å°æ¯”è¾ƒï¼ˆåœ¨posterèµ‹å€¼ä¹‹åï¼‰
        if is_sod_studio and res.poster and res.thumb:
            ps_url = res.poster  # ps.jpg
            pl_url = res.thumb  # pl.jpg
            try:
                ps_size = await self._get_url_content_length(ps_url)
                pl_size = await self._get_url_content_length(pl_url)

                if ps_size and pl_size:
                    if ps_size < pl_size * 0.5:
                        signal.add_log(
                            f"SODå·¥ä½œå®¤ps.jpgåˆ†è¾¨ç‡è¿‡ä½({ps_size}B) vs pl.jpg({pl_size}B)ï¼Œ"
                            f"å°†ä½¿ç”¨è£å‰ªåçš„å›¾ç‰‡è€Œä¸æ˜¯ç›´æ¥ä¸‹è½½"
                        )
                        res.image_download = "VR" in res.title
                    else:
                        signal.add_log(
                            f"æ£€æµ‹åˆ°SODå·¥ä½œå®¤: {res.studio}ï¼Œps.jpgåˆ†è¾¨ç‡å……è¶³({ps_size}B)ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹å›¾ç‰‡ä¸è¿›è¡Œè£å‰ª"
                        )
                else:
                    signal.add_log(f"æ£€æµ‹åˆ°SODå·¥ä½œå®¤: {res.studio}ï¼Œæ— æ³•è·å–å›¾ç‰‡å¤§å°ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹å›¾ç‰‡ä¸è¿›è¡Œè£å‰ª")
            except Exception as e:
                signal.add_log(f"SODå·¥ä½œå®¤å›¾ç‰‡å¤§å°æ¯”è¾ƒå¤±è´¥: {e}ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹å›¾ç‰‡ä¸è¿›è¡Œè£å‰ª")

        if not res.publisher:
            res.publisher = res.studio
        if len(res.release) >= 4:
            res.year = res.release[:4]
        return res

    @override
    async def _parse_detail_page(self, ctx, html: Selector, detail_url: str) -> CrawlerData | None:
        raise NotImplementedError
