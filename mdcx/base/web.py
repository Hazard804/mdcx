#!/usr/bin/env python3
import asyncio
import re
import threading
import time
from io import BytesIO
from pathlib import Path
from typing import Any, Literal, overload

import aiofiles.os
import httpx
from lxml import etree
from PIL import Image
from ping3 import ping

from ..config.manager import manager
from ..consts import GITHUB_RELEASES_API_LATEST
from ..manual import ManualConfig
from ..models.log_buffer import LogBuffer
from ..signals import signal
from ..utils import executor
from ..utils.file import check_pic_async


@overload
async def check_url(url: str, length: Literal[False] = False, real_url: bool = False) -> str | None: ...
@overload
async def check_url(url: str, length: Literal[True] = True, real_url: bool = False) -> int | None: ...
async def check_url(url: str, length: bool = False, real_url: bool = False):
    """
    æ£€æµ‹ä¸‹è½½é“¾æ¥. å¤±è´¥æ—¶è¿”å› None.

    Args:
        url (str): è¦æ£€æµ‹çš„ URL
        length (bool, optional): æ˜¯å¦è¿”å›æ–‡ä»¶å¤§å°. Defaults to False.
        real_url (bool, optional): ç›´æ¥è¿”å›çœŸå® URL ä¸è¿›è¡Œåç»­æ£€æŸ¥. Defaults to False.
    """
    if not url:
        return

    if "http" not in url:
        signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: æ ¼å¼é”™è¯¯ {url}")
        return

    # å¯¹äº AWS å›¾ç‰‡é“¾æ¥ï¼Œå¢åŠ é‡è¯•æ¬¡æ•°
    is_aws_image = "awsimgsrc.dmm.co.jp" in url
    max_retries = 3 if is_aws_image else 1

    for retry_attempt in range(max_retries):
        try:
            # åˆ¤æ–­æ˜¯å¦ä¸º awsimgsrc.dmm.co.jp å›¾ç‰‡é“¾æ¥
            if is_aws_image:
                # æ£€æŸ¥å‚æ•°æ˜¯å¦å·²å­˜åœ¨
                has_w = re.search(r"[?&]w=120(&|$)", url)
                has_h = re.search(r"[?&]h=90(&|$)", url)
                if not (has_w and has_h):
                    # æ‹¼æ¥å‚æ•°
                    if "?" in url:
                        url += "&w=120&h=90"
                    else:
                        url += "?&w=120&h=90"
                # ä½¿ç”¨ GET è¯·æ±‚
                response, error = await manager.computed.async_client.request("GET", url)
            else:
                # å…¶ä»–æƒ…å†µä½¿ç”¨ HEAD è¯·æ±‚
                response, error = await manager.computed.async_client.request("HEAD", url)

            # å¤„ç†è¯·æ±‚å¤±è´¥çš„æƒ…å†µ
            if response is None:
                if retry_attempt < max_retries - 1:
                    signal.add_log(f"ğŸŸ¡ æ£€æµ‹é“¾æ¥å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retry_attempt + 1}/{max_retries}): {error}")
                    await asyncio.sleep(1 + retry_attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: {error}")
                    return

            # ä¸è¾“å‡ºè·å– dmmé¢„è§ˆè§†é¢‘(trailer) æœ€é«˜åˆ†è¾¨ç‡çš„æµ‹è¯•ç»“æœåˆ°æ—¥å¿—ä¸­
            if response.status_code == 404 and "_w.mp4" in url:
                return

            # è¿”å›é‡å®šå‘çš„url
            true_url = str(response.url)
            if real_url:
                return true_url

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            if "login" in true_url:
                signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: éœ€ç™»å½• {true_url}")
                return

            # æ£€æŸ¥æ˜¯å¦å¸¦æœ‰å›¾ç‰‡ä¸å­˜åœ¨çš„å…³é”®è¯
            bad_url_keys = ["now_printing", "nowprinting", "noimage", "nopic", "media_violation"]
            for each_key in bad_url_keys:
                if each_key in true_url:
                    signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: å›¾ç‰‡å·²è¢«ç½‘ç«™åˆ é™¤ {url}")
                    return

            # è·å–æ–‡ä»¶å¤§å°
            content_length = response.headers.get("Content-Length")
            if not content_length:
                # å¦‚æœæ²¡æœ‰è·å–åˆ°æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä¸‹è½½æ•°æ®
                content, error = await manager.computed.async_client.get_content(true_url)

                if content is not None and len(content) > 0:
                    signal.add_log(f"âœ… æ£€æµ‹é“¾æ¥é€šè¿‡: é¢„ä¸‹è½½æˆåŠŸ {true_url}")
                    return 10240 if length else true_url
                else:
                    signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: æœªè¿”å›å¤§å°ä¸”é¢„ä¸‹è½½å¤±è´¥ {true_url}")
                    return
            # å¦‚æœè¿”å›å†…å®¹çš„æ–‡ä»¶å¤§å° < 8kï¼Œè§†ä¸ºä¸å¯ç”¨
            elif int(content_length) < 8192:
                # awsimgsrc.dmm.co.jp ä¸” GET è¯·æ±‚æ—¶è·³è¿‡å°äº8Kçš„æ£€æŸ¥
                if "awsimgsrc.dmm.co.jp" in true_url and getattr(response.request, "method", None) == "GET":
                    signal.add_log(f"âœ… æ£€æµ‹é“¾æ¥é€šè¿‡: awsimgsrc å°å›¾ {true_url}")
                    return int(content_length) if length else true_url.replace("w=120&h=90", "")
                signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: è¿”å›å¤§å°({content_length}) < 8k {true_url}")
                return

            signal.add_log(f"âœ… æ£€æµ‹é“¾æ¥é€šè¿‡: è¿”å›å¤§å°({content_length}) {true_url}")
            return int(content_length) if length else true_url

        except Exception as e:
            if retry_attempt < max_retries - 1:
                signal.add_log(f"ğŸŸ¡ æ£€æµ‹é“¾æ¥å¼‚å¸¸ï¼Œæ­£åœ¨é‡è¯• ({retry_attempt + 1}/{max_retries}): {e}")
                await asyncio.sleep(1 + retry_attempt)
                continue
            else:
                signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: æœªçŸ¥å¼‚å¸¸ {e} {url}")
                return


async def get_avsox_domain() -> str:
    issue_url = "https://tellme.pw/avsox"
    response, error = await manager.computed.async_client.get_text(issue_url)
    domain = "https://avsox.click"
    if response is not None:
        res = re.findall(r'(https://[^"]+)', response)
        for s in res:
            if s and "https://avsox.com" not in s or "api.qrserver.com" not in s:
                return s
    return domain


async def get_amazon_data(req_url: str) -> tuple[bool, str]:
    """
    è·å– Amazon æ•°æ®
    """
    headers = {
        "accept-encoding": "gzip, deflate, br",
        "accept-language": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
        "Host": "www.amazon.co.jp",
    }
    html_info, error = await manager.computed.async_client.get_text(req_url, headers=headers, encoding="utf-8")
    if html_info is None:
        html_info, error = await manager.computed.async_client.get_text(req_url, headers=headers, encoding="utf-8")
    if html_info is None:
        session_id = ""
        ubid_acbjp = ""
        if x := re.findall(r'sessionId: "([^"]+)', html_info or ""):
            session_id = x[0]
        if x := re.findall(r"ubid-acbjp=([^ ]+)", html_info or ""):
            ubid_acbjp = x[0]
        headers_o = {
            "cookie": f"session-id={session_id}; ubid_acbjp={ubid_acbjp}",
        }
        headers.update(headers_o)
        html_info, error = await manager.computed.async_client.get_text(req_url, headers=headers, encoding="utf-8")
    if html_info is None:
        return False, error
    if "HTTP 503" in html_info:
        headers = {
            "accept-language": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
            "Host": "www.amazon.co.jp",
        }
        html_info, error = await manager.computed.async_client.get_text(req_url, headers=headers, encoding="utf-8")
    if html_info is None:
        return False, error
    return True, html_info


async def get_imgsize(url) -> tuple[int, int]:
    response, _ = await manager.computed.async_client.request("GET", url, stream=True)
    if response is None or response.status_code != 200:
        return 0, 0
    file_head = BytesIO()
    chunk_size = 1024 * 10
    try:
        for chunk in response.iter_content(chunk_size):
            file_head.write(await chunk)
            try:

                def _get_size():
                    with Image.open(file_head) as img:
                        return img.size

                return await asyncio.to_thread(_get_size)
            except Exception:
                # å¦‚æœè§£æå¤±è´¥ï¼Œç»§ç»­ä¸‹è½½æ›´å¤šæ•°æ®
                continue
    except Exception:
        return 0, 0
    finally:
        response.close()

    return 0, 0


async def get_dmm_trailer(trailer_url: str) -> str:
    """
    å°è¯•è·å– dmm æœ€é«˜åˆ†è¾¨ç‡é¢„å‘Šç‰‡.

    Returns:
        str: æœ‰æ•ˆçš„æœ€é«˜åˆ†è¾¨ç‡é¢„å‘Šç‰‡ URL.
    """
    # å¦‚æœä¸æ˜¯ DMM åŸŸååˆ™ç›´æ¥è¿”å›
    if ".dmm.co" not in trailer_url:
        return trailer_url

    # å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
    if trailer_url.startswith("//"):
        trailer_url = "https:" + trailer_url

    # å¤„ç†ä¸´æ—¶é“¾æ¥æ ¼å¼ï¼ˆ/pv/{temp_key}/{filename}ï¼‰ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    # ä¸´æ—¶é“¾æ¥ç¤ºä¾‹: https://cc3001.dmm.co.jp/pv/{temp_key}/asfb00192_mhb_w.mp4
    # ä¸´æ—¶é“¾æ¥ç¤ºä¾‹: https://cc3001.dmm.co.jp/pv/{temp_key}/1start4814k.mp4
    # ä¸´æ—¶é“¾æ¥ç¤ºä¾‹: https://cc3001.dmm.co.jp/pv/{temp_key}/n_707agvn001_dmb_w.mp4
    # æ ‡å‡†æ ¼å¼ç¤ºä¾‹: https://cc3001.dmm.co.jp/litevideo/freepv/a/asf/asfb00192/asfb00192_mhb_w.mp4
    if "/pv/" in trailer_url:
        signal.add_log("ğŸ”„ æ£€æµ‹åˆ°ä¸´æ—¶é¢„å‘Šç‰‡é“¾æ¥ï¼Œå¼€å§‹è½¬æ¢...")
        filename_match = re.search(r"/pv/[^/]+/(.+?)(?:\.mp4)?$", trailer_url)
        if filename_match:
            filename_base = filename_match.group(1).replace(".mp4", "")
            # å»æ‰è´¨é‡æ ‡è®°åç¼€
            # 1) æ—§æ ¼å¼: _mhb_w / _hhb_w / _4k_w / _dmb_h / _sm_s ç­‰
            # 2) æ–°æ ¼å¼: hhb / mhb / dmb / dm / smï¼ˆæ—  _w/_s åç¼€ï¼‰
            cid = re.sub(r"(_[a-z0-9]+_[a-z])?$", "", filename_base, flags=re.IGNORECASE)
            cid = re.sub(r"(hhb|mhb|dmb|dm|sm|4k)$", "", cid, flags=re.IGNORECASE)
            # ç¡®ä¿æå–åˆ°çš„æ˜¯æœ‰æ•ˆçš„äº§å“IDï¼ˆåŒ…å«å­—æ¯å’Œæ•°å­—ï¼‰
            if re.search(r"[a-z]", cid, re.IGNORECASE) and re.search(r"\d", cid):
                prefix = cid[0]
                three_char = cid[:3]
                converted_url = (
                    f"https://cc3001.dmm.co.jp/litevideo/freepv/{prefix}/{three_char}/{cid}/{filename_base}.mp4"
                )
                signal.add_log(f"ğŸ“ è½¬æ¢åçš„URL: {converted_url}")
                # å°è¯•éªŒè¯è½¬æ¢åçš„URLï¼Œæœ€å¤šé‡è¯•3æ¬¡ï¼ˆä»…å¯¹é404é”™è¯¯é‡è¯•ï¼‰
                for attempt in range(3):
                    try:
                        # è¿›è¡ŒHEADè¯·æ±‚æ£€æµ‹
                        response, error = await manager.computed.async_client.request("HEAD", converted_url)

                        if response is not None:
                            # è¯·æ±‚æˆåŠŸ
                            if response.status_code == 404:
                                # 404é”™è¯¯è¯´æ˜è½¬æ¢åçš„URLä¸å­˜åœ¨ï¼Œå›é€€åˆ°åŸå§‹URL
                                signal.add_log("âš ï¸ è½¬æ¢åçš„URLè¿”å›404ï¼Œå›é€€åˆ°åŸå§‹é“¾æ¥")
                                break
                            elif 200 <= response.status_code < 300:
                                # 2xxæˆåŠŸï¼Œä½¿ç”¨è½¬æ¢åçš„URL
                                signal.add_log(f"âœ… è½¬æ¢åçš„URLéªŒè¯æˆåŠŸ (HTTP {response.status_code})")
                                trailer_url = converted_url
                                break
                            else:
                                # å…¶ä»–4xx/5xxé”™è¯¯ï¼Œç»§ç»­é‡è¯•
                                retry_msg = (
                                    f"ğŸŸ¡ è½¬æ¢åçš„URLæ£€æµ‹å¤±è´¥ (HTTP {response.status_code})ï¼Œ"
                                    f"å‡†å¤‡é‡è¯• ({attempt + 1}/3)..."
                                )
                                signal.add_log(retry_msg)
                                if attempt < 2:
                                    await asyncio.sleep(0.5 * (attempt + 1))
                                    continue
                                else:
                                    # é‡è¯•3æ¬¡ä»å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹URL
                                    signal.add_log("âš ï¸ é‡è¯•3æ¬¡åä»å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é“¾æ¥")
                                    break
                        else:
                            # æ£€æŸ¥æ˜¯å¦ä¸º 404 é”™è¯¯
                            if "404" in str(error):
                                # 404é”™è¯¯è¯´æ˜è½¬æ¢åçš„URLä¸å­˜åœ¨ï¼Œç›´æ¥å›é€€
                                signal.add_log("âš ï¸ è½¬æ¢åçš„URLè¿”å›404ï¼Œå›é€€åˆ°åŸå§‹é“¾æ¥")
                                break
                            else:
                                # å…¶ä»–ç½‘ç»œé”™è¯¯ã€è¶…æ—¶ç­‰ï¼Œé‡è¯•
                                signal.add_log(f"ğŸŸ¡ è½¬æ¢åçš„URLç½‘ç»œé”™è¯¯: {error}ï¼Œå‡†å¤‡é‡è¯• ({attempt + 1}/3)...")
                                if attempt < 2:
                                    await asyncio.sleep(0.5 * (attempt + 1))
                                    continue
                                else:
                                    # é‡è¯•3æ¬¡ä»å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹URL
                                    signal.add_log("âš ï¸ é‡è¯•3æ¬¡åä»å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é“¾æ¥")
                                    break
                    except Exception as e:
                        # å¼‚å¸¸å¤„ç†ï¼Œç»§ç»­é‡è¯•
                        signal.add_log(f"ğŸŸ¡ è½¬æ¢åçš„URLå¼‚å¸¸: {e}ï¼Œå‡†å¤‡é‡è¯• ({attempt + 1}/3)...")
                        if attempt < 2:
                            await asyncio.sleep(0.5 * (attempt + 1))
                            continue
                        else:
                            # é‡è¯•3æ¬¡ä»å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹URL
                            signal.add_log("âš ï¸ é‡è¯•3æ¬¡åä»å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é“¾æ¥")
                            break

    """
    DMM é¢„è§ˆç‰‡åˆ†è¾¨ç‡å¯¹åº”å…³ç³»ï¼ˆæ—§æ ¼å¼ï¼‰:
    '_sm_w.mp4': 320*180, 3.8MB     # æœ€ä½åˆ†è¾¨ç‡
    '_dm_w.mp4': 560*316, 10.1MB    # ä¸­ç­‰åˆ†è¾¨ç‡
    '_dmb_w.mp4': 720*404, 14.6MB   # æ¬¡é«˜åˆ†è¾¨ç‡
    '_mhb_w.mp4': 720*404, 27.9MB
    '_hhb_w.mp4': æ›´é«˜ç ç‡ï¼ˆå¸¸è§çº¦ 60MBï¼‰
    '_4k_w.mp4': æœ€é«˜åˆ†è¾¨ç‡

    æ—§æ ¼å¼å…¶ä»–å¯èƒ½çš„åç¼€: _s, _hï¼ˆå¦‚ _sm_s.mp4, _dmb_h.mp4ï¼‰

    DMM é¢„è§ˆç‰‡åˆ†è¾¨ç‡å¯¹åº”å…³ç³»ï¼ˆæ–°æ ¼å¼ï¼‰:
    'sm.mp4'  < 'dm.mp4' < 'dmb.mp4' < 'mhb.mp4' < 'hhb.mp4' < '4k.mp4'
    å¸¸è§ç¤ºä¾‹: nima00070sm.mp4 / nima00070dm.mp4 / nima00070dmb.mp4 / nima00070mhb.mp4 / nima00070hhb.mp4 / nima000704k.mp4

    ç¤ºä¾‹:
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_sm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dmb_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_mhb_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_hhb_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_4k_w.mp4
    https://cc3001.dmm.co.jp/pv/xxxx/nima00070mhb.mp4
    https://cc3001.dmm.co.jp/pv/xxxx/nima00070hhb.mp4
    https://cc3001.dmm.co.jp/pv/xxxx/nima000704k.mp4
    """

    # æ—§æ ¼å¼ï¼š..._sm_w.mp4 / ..._dmb_h.mp4
    if matched := re.search(r"(.+)_([a-z0-9]+)_([a-z])\.mp4$", trailer_url, flags=re.IGNORECASE):
        base_url, quality_level, suffix_char = matched.groups()
        quality_level = quality_level.lower()
        suffix_char = suffix_char.lower()
        quality_levels = ("sm", "dm", "dmb", "mhb", "hhb", "4k")

        if quality_level in quality_levels:
            current_index = quality_levels.index(quality_level)
            suffix_candidates = (suffix_char,) + tuple(s for s in ("w", "s", "h") if s != suffix_char)
            for i in range(len(quality_levels) - 1, current_index, -1):
                higher_quality = quality_levels[i]
                for test_suffix_char in suffix_candidates:
                    test_url = base_url + f"_{higher_quality}_{test_suffix_char}.mp4"
                    if await check_url(test_url):
                        signal.add_log(
                            f"ğŸ¬ DMM trailer å‡çº§(æ—§æ ¼å¼): {quality_level}_{suffix_char} -> "
                            f"{higher_quality}_{test_suffix_char}"
                        )
                        signal.add_log(f"ğŸ¬ DMM trailer URL: {trailer_url} -> {test_url}")
                        return test_url
            signal.add_log(f"ğŸ¬ DMM trailer ä¿æŒåŸè´¨é‡(æ—§æ ¼å¼): {quality_level}_{suffix_char} {trailer_url}")
        return trailer_url

    # æ–°æ ¼å¼ï¼š...nima00070mhb.mp4 / ...nima00070hhb.mp4ï¼ˆæ—  _w/_s åç¼€ï¼‰
    if matched := re.search(r"(.+?)(sm|dm|dmb|mhb|hhb|4k)\.mp4$", trailer_url, flags=re.IGNORECASE):
        base_url, quality_level = matched.groups()
        quality_level = quality_level.lower()
        quality_levels = ("sm", "dm", "dmb", "mhb", "hhb", "4k")

        if quality_level in quality_levels:
            current_index = quality_levels.index(quality_level)
            for i in range(len(quality_levels) - 1, current_index, -1):
                higher_quality = quality_levels[i]
                test_url = base_url + f"{higher_quality}.mp4"
                if await check_url(test_url):
                    signal.add_log(f"ğŸ¬ DMM trailer å‡çº§(æ–°æ ¼å¼): {quality_level} -> {higher_quality}")
                    signal.add_log(f"ğŸ¬ DMM trailer URL: {trailer_url} -> {test_url}")
                    return test_url
            signal.add_log(f"ğŸ¬ DMM trailer ä¿æŒåŸè´¨é‡(æ–°æ ¼å¼): {quality_level} {trailer_url}")

    return trailer_url


def _ping_host_thread(host_address: str, result_list: list[int | None], i: int) -> None:
    response = ping(host_address, timeout=1)
    result_list[i] = int(response * 1000) if response else 0


# todo å¯ä»¥ç§»é™¤ ping, ä»…é  http request æ£€æµ‹ç½‘ç»œè¿é€šæ€§
def ping_host(host_address: str) -> str:
    count = manager.config.retry
    result_list: list[int | None] = [None] * count
    thread_list: list[threading.Thread] = [None] * count  # type: ignore
    for i in range(count):
        thread_list[i] = threading.Thread(target=_ping_host_thread, args=(host_address, result_list, i))
        thread_list[i].start()
    for i in range(count):
        thread_list[i].join()
    new_list = [each for each in result_list if each]
    return (
        f"  â± Ping {int(sum(new_list) / len(new_list))} ms ({len(new_list)}/{count})"
        if new_list
        else f"  ğŸ”´ Ping - ms (0/{count})"
    )


def check_version() -> int | None:
    if manager.config.update_check:
        url = GITHUB_RELEASES_API_LATEST
        headers = {
            "Accept": "application/vnd.github+json",
            "User-Agent": "mdcx-update-check",
            "X-GitHub-Api-Version": "2022-11-28",
        }
        timeout = max(float(manager.config.timeout), 5.0)
        configured_proxy = manager.config.proxy.strip() if manager.config.use_proxy and manager.config.proxy else ""
        request_proxies = [configured_proxy] if configured_proxy else []
        request_proxies.append("")

        last_error = ""
        for proxy in dict.fromkeys(request_proxies):
            try:
                client_kwargs: dict[str, Any] = {"timeout": timeout, "follow_redirects": True}
                if proxy:
                    client_kwargs["proxy"] = proxy
                with httpx.Client(**client_kwargs) as client:
                    response = client.get(url, headers=headers)
            except Exception as e:
                last_error = str(e)
                continue

            if response.status_code != 200:
                if response.status_code == 403 and response.headers.get("x-ratelimit-remaining") == "0":
                    reset_raw = response.headers.get("x-ratelimit-reset", "")
                    if reset_raw.isdigit():
                        reset_at = time.strftime("%H:%M:%S", time.localtime(int(reset_raw)))
                        last_error = f"GitHub API é™æµï¼ˆ403ï¼Œå‰©ä½™ 0ï¼Œé¢„è®¡é‡ç½® {reset_at}ï¼‰"
                    else:
                        last_error = "GitHub API é™æµï¼ˆ403ï¼Œå‰©ä½™ 0ï¼‰"
                else:
                    last_error = f"HTTP {response.status_code}"
                continue

            try:
                latest_version = int(str(response.json()["tag_name"]).strip())
                return latest_version
            except Exception:
                signal.add_log(f"âŒ è·å–æœ€æ–°ç‰ˆæœ¬å¤±è´¥ï¼{response.text}")
                return None

        if last_error:
            signal.add_log(f"âŒ è·å–æœ€æ–°ç‰ˆæœ¬å¤±è´¥ï¼{last_error}")
    return None


def check_theporndb_api_token() -> str:
    tips = "âœ… è¿æ¥æ­£å¸¸! "
    api_token = manager.config.theporndb_api_token
    url = "https://api.theporndb.net/scenes/hash/8679fcbdd29fa735"
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }
    if not api_token:
        tips = "âŒ æœªå¡«å†™ API Tokenï¼Œå½±å“æ¬§ç¾åˆ®å‰Šï¼å¯åœ¨ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€æ·»åŠ ï¼"
    else:
        response, err = executor.run(manager.computed.async_client.request("GET", url, headers=headers))
        if response is None:
            tips = f"âŒ ThePornDB è¿æ¥å¤±è´¥: {err}"
            signal.show_log_text(tips)
            return tips
        if response.status_code == 401 and "Unauthenticated" in str(response.text):
            tips = "âŒ API Token é”™è¯¯ï¼å½±å“æ¬§ç¾åˆ®å‰Šï¼è¯·åˆ°ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€ä¸­ä¿®æ”¹ã€‚"
        elif response.status_code == 200:
            tips = "âœ… è¿æ¥æ­£å¸¸ï¼" if response.json().get("data") else "âŒ è¿”å›æ•°æ®å¼‚å¸¸ï¼"
        else:
            tips = f"âŒ è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {response.status_code} {response.text}"
    signal.show_log_text(tips.replace("âŒ", " âŒ ThePornDB").replace("âœ…", " âœ… ThePornDB"))
    return tips


async def _get_pic_by_google(pic_url):
    google_keyused = manager.computed.google_keyused
    google_keyword = manager.computed.google_keyword
    req_url = f"https://www.google.com/searchbyimage?sbisrc=2&image_url={pic_url}"
    # req_url = f'https://lens.google.com/uploadbyurl?url={pic_url}&hl=zh-CN&re=df&ep=gisbubu'
    response, error = await manager.computed.async_client.get_text(req_url)
    big_pic = True
    if response is None:
        return "", (0, 0), False
    url_list = re.findall(r'a href="([^"]+isz:l[^"]+)">', response)
    url_list_middle = re.findall(r'a href="([^"]+isz:m[^"]+)">', response)
    if not url_list and url_list_middle:
        url_list = url_list_middle
        big_pic = False
    if url_list:
        req_url = "https://www.google.com" + url_list[0].replace("amp;", "")
        response, error = await manager.computed.async_client.get_text(req_url)
    if response is None:
        return "", (0, 0), False
    url_list = re.findall(r'\["(http[^"]+)",(\d{3,4}),(\d{3,4})\],[^[]', response)
    # ä¼˜å…ˆä¸‹è½½æ”¾å‰é¢
    new_url_list = []
    for each_url in url_list.copy():
        if int(each_url[2]) < 800:
            url_list.remove(each_url)

    for each_key in google_keyused:
        for each_url in url_list.copy():
            if each_key in each_url[0]:
                new_url_list.append(each_url)
                url_list.remove(each_url)
    # åªä¸‹è½½å…³æ—¶ï¼Œè¿½åŠ å‰©ä½™åœ°å€
    if "goo_only" not in [item.value for item in manager.config.download_hd_pics]:
        new_url_list += url_list
    # è§£æåœ°å€
    for each in new_url_list:
        temp_url = each[0]
        for temp_keyword in google_keyword:
            if temp_keyword in temp_url:
                break
        else:
            h = int(each[1])
            w = int(each[2])
            if w > h and w / h < 1.4:  # thumb è¢«æ‹‰é«˜æ—¶è·³è¿‡
                continue

            p_url = temp_url.encode("utf-8").decode("unicode_escape")  # urlä¸­çš„Unicodeå­—ç¬¦è½¬ä¹‰ï¼Œä¸è½¬ä¹‰ï¼Œurlè¯·æ±‚ä¼šå¤±è´¥
            if "m.media-amazon.com" in p_url:
                p_url = re.sub(r"\._[_]?AC_[^\.]+\.", ".", p_url)
                pic_size = await get_imgsize(p_url)
                if pic_size[0]:
                    return p_url, pic_size, big_pic
            else:
                url = await check_url(p_url)
                if url:
                    pic_size = (w, h)
                    return url, pic_size, big_pic
    return "", (0, 0), False


async def get_big_pic_by_google(pic_url, poster=False) -> tuple[str, tuple[int, int]]:
    url, pic_size, big_pic = await _get_pic_by_google(pic_url)
    if not poster:
        if big_pic or (
            pic_size and int(pic_size[0]) > 800 and int(pic_size[1]) > 539
        ):  # cover æœ‰å¤§å›¾æ—¶æˆ–è€…å›¾ç‰‡é«˜åº¦ > 800 æ—¶ä½¿ç”¨è¯¥å›¾ç‰‡
            return url, pic_size
        return "", (0, 0)
    if url and int(pic_size[1]) < 1000:  # posterï¼Œå›¾ç‰‡é«˜åº¦å°äº 1500ï¼Œé‡æ–°æœç´¢ä¸€æ¬¡
        url, pic_size, big_pic = await _get_pic_by_google(url)
    if pic_size and (
        big_pic or "blogger.googleusercontent.com" in url or int(pic_size[1]) > 560
    ):  # posterï¼Œå¤§å›¾æˆ–é«˜åº¦ > 560 æ—¶ï¼Œä½¿ç”¨è¯¥å›¾ç‰‡
        return url, pic_size
    else:
        return "", (0, 0)


async def get_actorname(number: str) -> tuple[bool, str]:
    # è·å–çœŸå®æ¼”å‘˜åå­—
    url = f"https://av-wiki.net/?s={number}"
    res, error = await manager.computed.async_client.get_text(url)
    if res is None:
        return False, f"Error: {error}"
    html_detail = etree.fromstring(res, etree.HTMLParser(encoding="utf-8"))
    actor_box = html_detail.xpath('//ul[@class="post-meta clearfix"]')
    for each in actor_box:
        actor_name = each.xpath('li[@class="actress-name"]/a/text()')
        actor_number = each.xpath('li[@class="actress-name"]/following-sibling::li[last()]/text()')
        if actor_number and (
            actor_number[0].upper().endswith(number.upper()) or number.upper().endswith(actor_number[0].upper())
        ):
            return True, ",".join(actor_name)
    return False, "No Result!"


async def get_yesjav_title(movie_number: str) -> str:
    yesjav_url = f"http://www.yesjav101.com/search.asp?q={movie_number}&"
    movie_title = ""
    response, error = await manager.computed.async_client.get_text(yesjav_url)
    if response is not None:
        parser = etree.HTMLParser(encoding="utf-8")
        html = etree.HTML(response, parser)
        movie_title = html.xpath(
            '//dl[@id="zi"]/p/font/a/b[contains(text(), $number)]/../../a[contains(text(), "ä¸­æ–‡å­—å¹•")]/text()',
            number=movie_number,
        )
        if movie_title:
            movie_title = movie_title[0]
            for each in ManualConfig.CHAR_LIST:
                movie_title = movie_title.replace(each, "")
            movie_title = movie_title.strip()
    return movie_title


async def download_file_with_filepath(url: str, file_path: Path, folder_new_path: Path) -> bool:
    if not url:
        return False

    if not await aiofiles.os.path.exists(folder_new_path):
        await aiofiles.os.makedirs(folder_new_path)
    try:
        if await manager.computed.async_client.download(url, file_path):
            return True
    except Exception:
        pass
    LogBuffer.log().write(f"\n ğŸ¥º Download failed! {url}")
    return False


async def download_extrafanart_task(task: tuple[str, Path, Path, str]) -> bool:
    extrafanart_url, extrafanart_file_path, extrafanart_folder_path, extrafanart_name = task
    if await download_file_with_filepath(extrafanart_url, extrafanart_file_path, extrafanart_folder_path):
        if await check_pic_async(extrafanart_file_path):
            return True
    else:
        LogBuffer.log().write(f"\n ğŸ’¡ {extrafanart_name} download failed! ( {extrafanart_url} )")
    return False
